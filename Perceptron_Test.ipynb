{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Perceptron Test example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by https://www.kdnuggets.com/2018/09/6-steps-write-machine-learning-algorithm.html\n",
    "and https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The single layer Perceptron is the most basic neural network. It’s typically used for binary classification problems (1 or 0, “yes” or “no”).\n",
    "* It’s a linear classifier, so it can only really be used when there’s a linear decision boundary. Some simple uses might be sentiment analysis (positive or negative response) or loan default prediction (“will default”, “will not default”). For both cases, the decision boundary would need to be linear.\n",
    "* If the decision boundary is non-linear, you really can’t use the Perceptron. For those problems, you’ll need to use something different.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Perceptron algorithm broken down into the following chunks:\n",
    "\n",
    "- Initialize the weights\n",
    "- Multiply weights by inputs and then sum them up (i.e. this is a 'dot-product' calculation)\n",
    "- Compare the result against the threshold to compute the output (0 or 1)\n",
    "- Update the weights\n",
    "- Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start learning a simple function. For a perceptron a NAND function is a perfect example. If both inputs are true (1) then the output is false (0), otherwise, the output is true. Here is what the data set looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<table align=\"left\" style=\"width:20%\">\n",
    "  <tr>\n",
    "      <th>x<sub>1</sub></th>\n",
    "      <th>x<sub>2</sub></th>\n",
    "    <th>y</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td> <td>0</td> <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td> <td>1</td> <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td> <td>0</td> <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td> <td>1</td> <td>0</td>\n",
    "  </tr>\n",
    "</table>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start implementating algorithm defined above. We can import and use the dot product algorithm from numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w = [0, 0, 0]\n",
    "x = [1, 0, 1]\n",
    "\n",
    "np.dot(w, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement minimal perceptron code to learn the NAND function. We can estimate the weight values for our training data using the stochastic gradient descent algorithm.\n",
    "\n",
    "Stochastic gradient descent requires two parameters:\n",
    "\n",
    "- Learning Rate: Used to limit the amount each weight is corrected each time it is updated.\n",
    "- Epochs: The number of times to run through the training data while updating the weight.\n",
    "These, along with the training data will be the arguments to the function.\n",
    "\n",
    "There are 3 loops we need to perform in the function:\n",
    "\n",
    "1. Loop over each epoch.\n",
    "2. Loop over each row in the training data for an epoch.\n",
    "3. Loop over each weight and update it for a row in an epoch.\n",
    "As you can see, we update each weight for each row in the training data, each epoch.\n",
    "\n",
    "Weights are updated based on the error the model made. The error is calculated as the difference between the expected output value and the prediction made with the candidate weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> epoch=0, lrate=0.100, error=1.000\n",
      "> epoch=1, lrate=0.100, error=3.000\n",
      "> epoch=2, lrate=0.100, error=3.000\n",
      "> epoch=3, lrate=0.100, error=2.000\n",
      "> epoch=4, lrate=0.100, error=1.000\n",
      "> epoch=5, lrate=0.100, error=0.000\n",
      "> epoch=6, lrate=0.100, error=0.000\n",
      "> epoch=7, lrate=0.100, error=0.000\n",
      "> epoch=8, lrate=0.100, error=0.000\n",
      "> epoch=9, lrate=0.100, error=0.000\n",
      "Final weights model =  [0.2, -0.2, -0.1]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row)-1):\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return 1.0 if activation >= 0.0 else 0.0\n",
    "\n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "    weights = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0.0\n",
    "        for row in train:\n",
    "            prediction = predict(row, weights)\n",
    "            error = row[-1] - prediction\n",
    "            sum_error += error**2\n",
    "            weights[0] = weights[0] + l_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "        print('> epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    return weights\n",
    "\n",
    "\n",
    "# Define training dataset \n",
    "#       NAND function\n",
    "#           x1,x2,y\n",
    "dataset = [ [0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,0]]\n",
    "\n",
    "# Calculate weights\n",
    "l_rate = 0.1\n",
    "n_epoch = 10\n",
    "weights = train_weights(dataset, l_rate, n_epoch)\n",
    "print('Final weights model = ', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make one-off predictions using our weights model\n",
    "predict(row=[0,0,None], weights=[0.2, -0.2, -0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(row=[0,1,None], weights=[0.2, -0.2, -0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(row=[1,0,None], weights=[0.2, -0.2, -0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(row=[1,1,None], weights=[0.2, -0.2, -0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is cool! We can see it takes 5 epochs for the weights to completely reduce the error to 0.0 and to learn the function. We note the final values of our weights (i.e. the model) and can re-use them to make predictions across a range of inputs.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do more here, like modify the learning rate or learn a different function but this is a good start! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with sklearn Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory... we should be able to test our perceptron implementation with the version implemented in the excellent scikit-learn libraries. If our implememntation is correct then we should get identical results, right ? Let's test it...!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset =  [[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 0]]\n",
      "x_train =  [[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]]\n",
      "y_train =  [1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# The input format of our data is slightly different\n",
    "# Each row in the training dataset has to have an extra value '1' pre-pended\n",
    "# This dummy feature tells the sklearn.Perceptron algorithm that the data has been centered\n",
    "# see sklearn docs (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)\n",
    "x_train = []\n",
    "y_train = []\n",
    "for d in dataset:\n",
    "    one_list = [1]    # insert dummy value '1'\n",
    "    one_list = one_list + d[0:-1]\n",
    "    x_train.append(one_list)\n",
    "    y_train = y_train + [d[-1]]   \n",
    "    \n",
    "# Training predictions are split off into a separate list (y_train)    \n",
    "    \n",
    "print('dataset = ', dataset)    \n",
    "print('x_train = ', x_train)\n",
    "print('y_train = ', y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test =  [[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]]\n",
      "y_predict =  [1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Train the sklearn Perceptron\n",
    "# eta0 is the learning rate \n",
    "# max_iter is the number of epochs\n",
    "# All other parameters are set to false\n",
    "clf = Perceptron(random_state=None, eta0=l_rate, shuffle=False, fit_intercept=False, max_iter=n_epoch)\n",
    "        \n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "x_test = x_train\n",
    "y_predict = clf.predict(x_test)\n",
    "print(\"x_test = \", x_test)\n",
    "print(\"y_predict = \", y_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our original perceptron implementation is correct then the weights from our original perceptron implementation and the weights from the sklearn Perceptron implementation should be the same..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.Perceptron weights:\n",
      "[ 0.2 -0.2 -0.1]\n",
      "my perceptron weights:\n",
      "[0.2, -0.2, -0.1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (\"sklearn.Perceptron weights:\")\n",
    "print (clf.coef_[0])\n",
    "\n",
    "print (\"my perceptron weights:\")\n",
    "print (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
