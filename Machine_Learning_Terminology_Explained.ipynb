{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Terminology Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanations of some of the terminology used in Machine Learning and Data Science.\n",
    "\n",
    "Useful formatting of Jupyter notebooks info here [http://jupyter.org/](http://jupyter.org/ \"http://jupyter.org/\")\n",
    "and [https://jupyter-notebook.readthedocs.io](https://jupyter-notebook.readthedocs.io \"https://jupyter-notebook.readthedocs.io\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<dl>\n",
    "\n",
    "  <dt>$A$ $priori$ association rules</dt> \n",
    "  <dd>The rules that can be observed in the training data\n",
    "    and, based on which, a classification of the future data can be made. Often used in Bayesian classification.</dd>\n",
    "\n",
    "  <dt>Bagging</dt>\n",
    "  <dd>A method of classifying a data item by the majority vote of the\n",
    "  classifiers trained on the random subsets of the training data.</dd>\n",
    "\n",
    "\n",
    "  <dt>Bayesian probability</dt>\n",
    "  <dd>Bayesian probability is an interpretation of the concept of probability, in which, instead of frequency or propensity of some phenomenon, probability is interpreted as reasonable expectation representing a state of knowledge or as quantification of a personal belief. The Bayesian interpretation of probability can be seen as an extension of propositional logic that enables reasoning with hypotheses, i.e., the propositions whose truth or falsity is uncertain. In the Bayesian view, a probability is assigned to a hypothesis, whereas under frequentist inference, a hypothesis is typically tested without being assigned a probability.\n",
    "\n",
    "Bayesian probability belongs to the category of evidential probabilities; to evaluate the probability of a hypothesis, the Bayesian probabilist specifies some <i>prior probability</i>, which is then updated to a <i>posterior probability</i> in the light of new, relevant data (evidence). The Bayesian interpretation provides a standard set of procedures and formulae to perform this calculation.\n",
    "\n",
    "The term Bayesian derives from the 18th century mathematician and theologian Thomas Bayes, who provided the first mathematical treatment of a non-trivial problem of statistical data analysis using what is now known as Bayesian inference. Mathematician Pierre-Simon Laplace pioneered and popularised what is now called Bayesian probability.</dd>\n",
    "\n",
    "\n",
    "  <dt>Correlation</dt>\n",
    "  <dd>The correlation coefficient measures the extent to which two variables are associated with one another. When high values of v1 go with high values of v2, v1 and v2 are positively associated. When high values of v1 are associated with low values of v2, v1 and v2 are negatively associated. The correlation coefficient is a standardized metric so that it always ranges from â€“1 (perfect negative correlation) to +1 (perfect positive correlation). A correlation coefficient of 0 indicates no correlation, but be aware that random arrangements of data will produce both positive and negative values for the correlation coefficient just by chance. Correlation does not imply causation.</dd>\n",
    "\n",
    "  <dt>Data</dt>\n",
    "  <dd>A collection of observations.</dd>\n",
    "\n",
    "  <dt>Decision Tree</dt> \n",
    "  <dd>A model classifying a data item into one of the classes at the leaf\n",
    "      node, based on the matching properties between the branches on the tree and the\n",
    "      actual data item.</dd>\n",
    "\n",
    "  <dt>Ensemble learning</dt> \n",
    "  <dd>A method of learning where different learning algorithms\n",
    "      are used to make a final conclusion.</dd>\n",
    "      \n",
    "\n",
    "  <dt>Fit</dt>\n",
    "  <dd>Applying a learning algorithm to data using analytical approaches.</dd>\n",
    "\n",
    "  <dt>Genetic algorithms</dt> \n",
    "  <dd>Machine learning algorithms inspired by the genetic\n",
    "  processes, for example, an evolution where classifiers with the best accuracy are\n",
    "  trained further.</dd>\n",
    "\n",
    "  <dt>Hyperparameters</dt>\n",
    "  <dd>The settings of a learning algorithm that need to be set before training.</dd>\n",
    "\n",
    "  <dt>k-Means Clustering algorithm</dt> \n",
    "  <dd>The clustering algorithm that divides the dataset into the k groups such that \n",
    "      the members in the group are as similar possible, that is, closest\n",
    "      to each other.</dd>\n",
    "\n",
    "  <dt>k-Nearest Neighbors algorithm</dt>\n",
    "  <dd>An algorithm that estimates an unknown data item to be like \n",
    "      the majority of the k-closest neighbors to that item.</dd>\n",
    "\n",
    "  <dt>Learning algorithms</dt>\n",
    "  <dd>An algorithm used to learn the best parameters of a model - for example, \n",
    "    linear regression, naive Bayes, or decision trees.</dd>\n",
    "\n",
    "  <dt>Loss</dt>\n",
    "  <dd>A metric to maximise or minimise through training.</dd>\n",
    "\n",
    "\n",
    "  <dt>Models</dt>\n",
    "  <dd>An output of a learning algorithm's training. Learning algorithms train models which we can then use to make predictions.   </dd>\n",
    "\n",
    "  <dt>Naive Bayes classifier</dt>\n",
    "  <dd>A way to classify a data item using Bayes' theorem about\n",
    "      the conditional probabilities,   \n",
    "   The naive aspect of the algorithm is based on its \n",
    "   assuming the independence between the given variables in the data.  </dd>\n",
    "\n",
    "   \\begin{equation*}\n",
    "         P(A|B)  =  \\frac{P(B|A) \\times P(A)}{P(B)}\n",
    "   \\end{equation*}\n",
    "\n",
    "\n",
    "   <dt>Neural networks</dt>\n",
    "   <dd>A machine learning algorithm consisting of a network of\n",
    "   simple classifiers making decisions based on the input or the results of the other\n",
    "   classifiers in the network.</dd>\n",
    "\n",
    "\n",
    "  <dt>Normalize Data (Normalization)</dt>\n",
    "  <dd>Normalization can refer to different techniques depending on context. Here, we use normalization\n",
    "to refer to rescaling an input variable to the range between 0 and 1. Normalization requires\n",
    "that you know the minimum and maximum values for each attribute being normalized.</dd>\n",
    "\n",
    "\n",
    "  <dt>Observation</dt>\n",
    "  <dd>A single unit in our level of observation - for example, a person, a sale, or a record.</dd>\n",
    "\n",
    "  <dt>Parameters</dt>\n",
    "  <dd>The weights or coefficients of a model learned through training.</dd>\n",
    "\n",
    "\n",
    "  <dt>Performance</dt>\n",
    "  <dd>A metric used to evaluate a model.</dd>\n",
    "\n",
    "  <dt>$Posterior$ probability</dt>\n",
    "  <dd>In Bayesian statistics, the posterior probability of a random event or an uncertain proposition is the conditional probability that is assigned after the relevant evidence or background is taken into account. Similarly, the posterior probability distribution is the probability distribution of an unknown quantity, treated as a random variable, conditional on the evidence obtained from an experiment or survey. \"Posterior\", in this context, means after taking into account the relevant evidence related to the particular case being examined.</dd>\n",
    "\n",
    "  <dt>Random Decision Tree</dt> \n",
    "  <dd>A decision tree in which every branch is formed using\n",
    "      only a random subset of the available variables during its construction.</dd>\n",
    "      \n",
    "  <dt>Random Forest</dt> \n",
    "  <dd>An ensemble of random decision trees constructed on the\n",
    "      random subset of the data with the replacement, where a data item is classified to\n",
    "      the class with the majority vote from its trees.</dd>  \n",
    "\n",
    "   <dt>Regression analysis</dt>\n",
    "   <dd>A method of the estimation of the unknown parameters in a\n",
    "       functional model predicting the output variable from the input variables, for\n",
    "       example, to estimate $m$ and $c$ in the linear model $y=m*x + c$.</dd>\n",
    "\n",
    "\n",
    "  <dt>Standardize Data (Standardization)</dt>\n",
    "  <dd>Standardization is a rescaling technique that refers to centering the distribution of the data on\n",
    "the value 0 and the standard deviation to the value 1. Together, the mean and the standard\n",
    "deviation can be used to summarize a normal distribution, also called the Gaussian distribution\n",
    "or bell curve. It requires that the mean and standard deviation of the values for each column be known\n",
    "prior to scaling.\n",
    "  </dd>  \n",
    "\n",
    "  <dt>Support Vector Machines (SVMs)</dt>\n",
    "  <dd>A classification algorithm that finds the maximum-margin hyperplane that divides the training \n",
    "      data into the given classes. This division by the hyperplane is then used to classify the  \n",
    "      data further.</dd>\n",
    "\n",
    "  <dt>Time series analysis</dt> \n",
    "  <dd>The analysis of data dependent on time; it mainly includes\n",
    "   the analysis of trend and seasonality.</dd>\n",
    "\n",
    "\n",
    "  <dt>Train</dt>\n",
    "  <dd>Applying a learning algorithm to data using numerical approaches like gradient-descent.</dd>\n",
    "\n",
    "</dl>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. When to Normalize and Standardize\n",
    "\n",
    "Standardization is a scaling technique that assumes your data conforms to a normal distribution.\n",
    "If a given data attribute is normal or close to normal, this is probably the scaling method to use.\n",
    "It is good practice to record the summary statistics used in the standardization process so that\n",
    "you can apply them when standardizing data in the future that you may want to use with your\n",
    "model. Normalization is a scaling technique that does not assume any specific distribution.\n",
    "\n",
    "If your data is not normally distributed, consider normalizing it prior to applying your\n",
    "machine learning algorithm. It is good practice to record the minimum and maximum values\n",
    "for each column used in the normalization process, again, in case you need to normalize new\n",
    "data in the future to be used with your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bayesian Probability Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example modified from here https://en.wikipedia.org/wiki/Posterior_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose there is a college having 60% male (M) and 40% female (F) students. The male students all wear trousers. The femal students wear skirts or trousers in equal numbers. An observer selects a random student from a distance: all the observer can see is that the student is wearing trousers. Given this knowledge, what is the probability this student is female? The correct answer can be computed using Bayes' theorem.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two events. Event $F$ is that the observed student is female. Event $T$ is that the observed student is wearing trousers. To compute the posterior probability P(F|T), we first need to know:\n",
    "\n",
    "- P(F), or the probability that the student is female regardless of any other information. Since the observer sees a random student, meaning that all students have the same probability of being observed, and the percentage of females among the students is 40%, so this probability equals 0.4.\n",
    "- P(M), or the probability that the student is not female (i.e. is male) regardless of any other information (M is the complementary event to F). This is 60%, or 0.6.\n",
    "- P(T|F), or the probability of the student wearing trousers given that the student is female. As they are as likely to wear skirts as trousers, this is 0.5.\n",
    "- P(T|M), or the probability of the student wearing trousers given that the student is male. This is given as 1.\n",
    "- P(T), or the probability of a (randomly selected) student wearing trousers regardless of any other information. We know that all males (i.e. 60% of students) wear trousers, and of all female students (i.e. 40% of students) half of them (i.e. 20%) wear trousers. So 60% plus 20% equals 80%, so this probability is 0.8. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having assigned probabilities, we can now plug the numbers into Bayes Formula to compute the posterior probabilities.\n",
    "Original formula,\n",
    "\n",
    "   \\begin{equation*}\n",
    "         P(A|B)  =  \\frac{P(B|A) \\times P(A)}{P(B)}\n",
    "   \\end{equation*}\n",
    "\n",
    "Original formula substituting with our events,\n",
    "\n",
    "   \\begin{equation*}\n",
    "         P(F|T)  =  \\frac{P(T|F) \\times P(F)}{P(T)}\n",
    "   \\end{equation*}\n",
    "\n",
    "Replace with assigned probabilities,\n",
    "\n",
    "   \\begin{equation*}\n",
    "                 =  \\frac{0.5 \\times 0.4}{0.8}\n",
    "   \\end{equation*}\n",
    "   \n",
    "   \\begin{equation*}   \n",
    "                 =  \\frac{0.2}{0.8}\n",
    "   \\end{equation*}\n",
    "   \n",
    "   \\begin{equation*}   \n",
    "                 =  0.25\n",
    "   \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition of this result is that out of every hundred students (60 males and 40 females), since we observe trousers the student is one of the 80 students who wear these (60 male and 20 female); since 20/80 = 1/4 of these are female, the probability that the student in trousers is a female is 1/4, or 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
